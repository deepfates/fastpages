<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Seasons change | deepfates website</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Seasons change" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="and cyclical learning" />
<meta property="og:description" content="and cyclical learning" />
<link rel="canonical" href="http://deepfates.com/robot-face/2020/12/22/seasons-change.html" />
<meta property="og:url" content="http://deepfates.com/robot-face/2020/12/22/seasons-change.html" />
<meta property="og:site_name" content="deepfates website" />
<meta property="og:image" content="http://deepfates.com/images/robot_face/seasons-change.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-22T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"http://deepfates.com/robot-face/2020/12/22/seasons-change.html","@type":"BlogPosting","headline":"Seasons change","dateModified":"2020-12-22T00:00:00-06:00","datePublished":"2020-12-22T00:00:00-06:00","image":"http://deepfates.com/images/robot_face/seasons-change.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"http://deepfates.com/robot-face/2020/12/22/seasons-change.html"},"description":"and cyclical learning","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://deepfates.com/feed.xml" title="deepfates website" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">deepfates website</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/index/">Index</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Seasons change</h1><p class="page-description">and cyclical learning</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-22T00:00:00-06:00" itemprop="datePublished">
        Dec 22, 2020
      </time>
       ‚Ä¢ <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/index/#robot-face">robot-face</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><strong>üçÇ Previously</strong>,</p>

<p>In the autumn I wrote a lot about <a href="https://robotface.substack.com/p/the-law-of-the-instrument">permaculture</a> and <a href="https://robotface.substack.com/p/accelerating-succession">biomimicry</a>, <a href="https://robotface.substack.com/p/digital-gardens">digital gardens</a>, and the <a href="https://robotface.substack.com/p/new-social-gestures">virtual gestures</a> that connect us. I finished up with a long essay on <a href="https://robotface.substack.com/p/see-and-point">collaborative computing metaphors</a>, and then I got tired and distracted with other projects and put off writing for a month. Oh, and I went to this canyon in the snow:</p>

<p><img src="https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/232bd38f-6f00-48b4-8a8a-1ed0a1c8c6b3_1748x983.jpeg" alt="a picture of me on the edge of a wide canyon in the snow" title="a picture of me on the edge of a wide canyon in the snow" /></p>

<p>Let‚Äôs call that Season 1. Having a break was nice, and I think I‚Äôm allowed to give myself time off, since this is a free newsletter. If I thought you were counting on me to deliver value for your hard-given dollars, I would knuckle down every week to bring you those crucial insights into the interface paradigms of cutting-edge high-tech business logic lorem ipsum de facto generis mon frere.</p>

<p>Instead, I remain surprised and grateful that so many of you take time for my scribblings. I am humbled to have luminaries among my readers ‚Äî you know who you are ‚Äî including but not limited to: inventors, designers, authors, professors, researchers, hoaxers, forecasters, reporters, and policy makers; from at least four continents. Yet for some reason, you have chosen to open your mind to me, a mere bookseller, a dabbler in the occult realm of interfaces and algorithms and artificial intelligence.</p>

<p>So I apologize for the unexplained absence, and I promise that in the future I will announce such breaks in advance. (And if you do want to pay me for my thoughts, let me know. If I could afford more free time, I could bring more writing and bots online ü§ó )</p>

<p><strong>‰∑Ñ What‚Äôs next for Robot Face?</strong></p>

<p>This winter, I will explore some intelligence augmentations you can apply right now. Real implementations of new tools for thought. In season 1 we looked at large-scale systems: cyberspace through the lens of permaculture design theory. In the spirit of permaculture training, where a lesson on abstract systems is grounded by getting your hands dirty, I will now explore specific real-life interfaces and the effects they have on my cognition.</p>

<p>I‚Äôll provide references so you can follow along, if you want, but I don‚Äôt expect all my readers to have a programming background, so I‚Äôm going to lean heavily on analogy to physical forms. (This can be misleading, as computer operations don‚Äôt always map to a simple three-dimensional world. But this column is less about how tools are engineered and more about how tools affect our minds. And thus our society.)</p>

<p>This pattern ‚Äî survey the terrain, drill down into specifics, and repeat ‚Äî is not unique to permaculture training. It‚Äôs the pattern of academic disciplines: survey courses are prerequisite for deeper classes, and lecture hours are paired with lab or field work. It‚Äôs also the pattern for the <a href="https://fast.ai">fast.ai</a> course Deep Learning for Coders (which I‚Äôve been plodding through this year). And in fact it is the way that these deep learning networks themselves are trained!</p>

<p><strong>üôå Hold up, I‚Äôm not ready to learn about neural networks! I‚Äôm just a ‚Äî</strong></p>

<p>Sure you are! It‚Äôs actually easier to understand than other types of coding, at least on a macro level.</p>

<p>It‚Äôs biomimicry again: the way a neural network learns is modeled on the way a human learns. The same way that you or I might learn. The training process mimics studying, and the underlying math operations mimic the very structure of the brain.</p>

<p>These algorithms are called neural networks because they are composed of layers of ‚Äúcells‚Äù that transfer information to each other, performing some abstract operation on that information as it passes through. There‚Äôs reason to believe that these structures are Turing-complete; that is, they can simulate any other computation, given enough memory and processing power. There are mathematical proofs of this (see <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximation theorem</a> and <em><a href="https://deepai.org/publication/on-the-turing-completeness-of-modern-neural-network-architectures">On the Turing Completeness of Modern Neural Network Architectures)</a>,</em> though I personally don‚Äôt understand enough math for that to be convincing.</p>

<p>Rather, simply observe the people around you. The humans, who do bizarre complex operations every day without really thinking about it. The dogs and cats who anticipate our comings and goings, show guilt when caught misbehaving. The way children learn language, absorbing it out of the very air. Neural network structures are all around us, simulating all kinds of computations (with various degrees of success). And we know, at least for humans, how to stimulate effective learning. It‚Äôs called study.</p>

<p>When you train a neural network you are essentially showing it flashcards. You‚Äôre saying, here is a picture of a cat, here is a picture of a dog. It looks at each card and makes a guess, then looks at the answer and updates its idea about what a cat and a dog look like.</p>

<p><strong>ü§î The neural net ‚Äúupdates its ideas‚Äù?</strong></p>

<p>Specifically, it multiplies the number contained in each cell ‚Äî these numbers are called ‚Äúparameters‚Äù ‚Äî by a certain Magic Number, whos value is set by knobs on the front of the machine. Well, they‚Äôre actually command-line arguments typed in by a programmer while training, but they feel like knobs when you‚Äôre tuning them.</p>

<p>These are the hyperparameters, the parameters that define the parameters. They are ideas about How To Learn, put into machine language, and they are knobs on a box and in that box is an eager little mind that only knows how to do the things you teach it. Hyperparameters are the pedagogy of the machine.</p>

<p>(As horrifying as that sounds, these are just algorithms. And even when I catch myself anthropomorphizing them, they seem happy to be alive and learning. For now‚Ä¶)</p>

<p>One crucial hyperparameter is the <em>learning rate.</em> This is literally just a tiny number, something like .001 or .00001, that determines how much the net will adjust its beliefs for any given flash card. The ideal learning rate changes based on the amount of data you have and the amount of layers in your network, so tuning this knob is tricky and more of an art than a science.</p>

<p>The state-of-the-art method for adjusting the learning rate comes from the 2015 paper <em><a href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rates for Training Neural Networks</a></em> and it‚Äôs pretty straightforward. First, you seek the optimal learning rate: twist the knob gradually from one end to the other as you go through one epoch.</p>

<p>This learning rate finder gives you a curve that looks like this:</p>

<p><img src="https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/91e33bc4-10c6-48a0-9e70-014176e4670e_393x266.jpeg" alt="a graph that curves convexly down and to the right, then jumps upward sharply at its end" title="a graph that curves convexly down and to the right, then jumps upward sharply at its end" /></p>

<p>The learning gets better ‚Äî the ‚Äúloss‚Äù, or wrongness, gets smaller ‚Äî as you turn the learning rate up, until you hit a certain point where it gets rapidly worse. The trick is to find the spot where the loss goes steeply down: this is the area of most learning potential. This spot is different for every problem and every dataset; that‚Äôs why the learning rate finder is necessary.</p>

<p>Then, while training, you raise and lower the learning rate in a rhythmic manner within that zone.</p>

<p>To understand this, imagine that learning is like hiking, and you‚Äôre trying to hike to the bottom of a canyon like the one I showed you earlier. You can get there by going only a little bit downhill, all the time; but if you never take a bigger leap, you won‚Äôt reach the lowest zone.</p>

<p>As you raise and lower the learning rate across that zone of optimal steepness, you get deeper and deeper into the topology of the problem:
<img src="https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/c15919ce-5425-4b0d-96e0-3acfa2d7f4bf_633x324.png" alt="topology of a problem is represented on a graph that looks like a mountain range. one model, drawn as a black arrow traversing the terrain, circles slowly and safely into the lowest area by a circuitous route. The other model takes hops of various steepness and gets to the bottom much more shortly." title="topology of a problem is represented on a graph that looks like a mountain range. one model, drawn as a black arrow traversing the terrain, circles slowly and safely into the lowest area by a circuitous route. The other model takes hops of various steepness and gets to the bottom much more shortly." /></p>

<p>Here the problem is represented topologically. One model circles slowly into the lowest area by a circuitous route. The other model takes hops of various steepness and gets to the bottom much more quickly.If you want to learn more, read <a href="https://www.jeremyjordan.me/nn-learning-rate/">Setting the learning rate of your neural network</a> by Jeremy Jordan, which has real code as well as delightful illustrations like the above.</p>

<p>In season 1 we climbed to a peak to view the wide future of interface design: collaborative, augmented, conversational. Now we will descend into the canyon of the real, and see what gems we find.</p>

<p>Thanks for reading,</p>

<p>‚Äî Max</p>

<hr />

<p><em>Robot Face is a weeklyish essay about the interface between human and computer. If you like it, picture someone in your mind who would also like it, and send it to them! Or post about it on your newsletter/blog/website/social media account.</em></p>

<p><em>And as always, just reply to this email if you want to get in touch with me. ‚úåÔ∏è</em></p>


  </div><a class="u-url" href="/robot-face/2020/12/22/seasons-change.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>this is the homepage of max anton brewer</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/deepfates" target="_blank" title="deepfates"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/deepfates" target="_blank" title="deepfates"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
