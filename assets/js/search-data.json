{
  
    
        "post0": {
            "title": "The Big Sleep",
            "content": "Previously, . I wrote about auto-completing language models — Transformers — and the feedback loop they create with our minds: . Robert Anton Wilson, a philosopher-entertainer of the 20th century, said “the border between the Real and the Unreal is not fixed, but just marks the last place where rival gangs of shamans fought each other to a standstill.” This is more true now than ever, because of the incredible power that intelligence augmentation will unleash. . Platforms will raise their language models with different values and those values will feed back into their decision-making apparatus. The worldviews encoded in the models will diverge, and our thoughts will be nudged by them. The tense border of Reality will erupt into a multifront war. . Perhaps this war has already begun. . I published those words on January 6th, at 10:16 my time. Two hours later, a wingnut known as Q Shaman stood at the podium of the United States Senate. . Think what you will of this encounter, it has permanently entered the narrative of this country and the world. Rival gangs of shamans fought each other to a standstill, and now we are in a new era. . I take no responsibility for this situation, of course. Lucky coincidence. Monkeys typing Hamlet, and all that. But imagine if somebody did: if they accidentally summoned Q Shaman with a stray word. . Likely it’s true! The mysterious “Q” ̶ı̶s̶ ̶J̶ı̶m̶ ̶W̶α̶t̶k̶ı̶n̶s̶  is officially anonymous, but is thought to be one person, or a small group of people, peddling misinformation to a wider ecosystem of grifters like Q Shaman. Whoever it is, they have a classic sorcerer’s apprentice problem. The conspiracy entertainment circuit can’t be turned off by just one person. . The war for reality is just beginning. . 🔮 Hello, I thought this was a newsletter about interfaces? Not politics and magic and whatever? . That’s the thing: politics and magic are interfaces too. Politics is a Turing machine that runs on human compute power, written in a language called Common Law. Magic — religion, shamanism, the deliberate manipulation of consensus reality — works on a lower level, exploiting basic hot-buttons of the human organism. . Now that computers can use human language, they can program us back. We need to think about magic, about human belief, when we think about something like Twitter or smartphones. Think not just about the technical aspects of the machine, but the technical aspects of humans as well. . For instance: human beings are visual creatures. our brains are easily fooled by our eyes. This is why things like deep fakes are so scary to people: they look real enough that the average human might just believe it. And with seven billion people in the world, what humans believe is almost as important as what is true.   . In that light, the news of OpenAI’s DALL-E and CLIP transformers – released on January 5 but overshadowed by the events of the next day – may be seen by future historians as more important than the Capitol riot itself.   . 🖇️What’s DALL-E and CLIP? . DALL-E is a GPT-3 variant that can generate images directly from text, and CLIP is the language model that tells it what to generate. DALL-E can do some pretty amazing things, and you can find some press about it. But as I mentioned last time, I think GPT-3 is too big to be practical, and I think CLIP is the more interesting development anyway.   . CLIP is actually two models combined: a vision transformer and a language transformer. At risk of oversimplifying this, the vision transformer translates each image into encoding that the language transformer can read, and then the language transforming attempts to predict that encoding directly. It’s trained on 400 million image/text pairs from the internet.   . CLIP is, in essence, a translator between visual language (of photographs, paintings, drawings, etc) and human language (in this case mostly English, I think). By itself it can classify images based on how close they are to its representation of a natural language sentence — for something like a natural language search engine of 2 million Unsplash images. or, combined with a sophisticated image generator, it can be used to tell the generator exactly what kind of image you want it to create.  . ⌚ Can I use it now? For some reason OpenAI has not given me an API key for DALL-E yet… . You can! The Big Sleep is a combination of CLIP and BigGAN, a modern generative adversarial network pre-trained to generate high-resolution images. It allows you to generate images without DALL-E, although the BigGAN generator isn’t as powerful and flexible as a GPT-3 model could be. You can use it right now in a Google Colab session from its creator @advadnoun. . I’ve been playing with it all week and it’s amazing the things it can come up with. N tural language is powerful because it has compositionality. It can express many different concepts and the ways they intersect. So the CLIP architecture is often looking at a concept it has never seen before in its entirety – this is called “zero shot transfer learning”. Take this picture, which it generated based on the phrase ”A painting of a powerful neutral spirit to balance any angels or demons i might have accidentally summoned into Twitter” . https://t.co/klAxM015zq . a.image2.image-link.image2-512-512 { padding-bottom: 100%; padding-bottom: min(100%, 512px); width: 100%; height: 0; } a.image2.image-link.image2-512-512 img { max-width: 512px; max-height: 512px; } It seems to have picked up on the color of the Twitter brand and its bird shaped outline, but inverted it to be a sun-bright bird in a Twitter-blue sky. With a Doge face: the ultimate arbiter of neutrality.  . The Big Sleep feels like an externalized version of lucid dreaming. Or the technique that Carl Jung called active imagination, in which you picture an image in your mind and let it evolve without dissipating. It’s a way to visualize the archetypes that live in the thought space of humanity. But active imagination is difficult, and personal.   . This tool puts it in your phone, makes a collective imagination accessible to your very word. it’s a scrying pool. And this new magic is like the old magic: if you know a thing’s true name, you can summon it.  . We are as wizards now. We have to get good at it.  . Thanks for reading, . — Max . a.image2.image-link.image2-512-512 { padding-bottom: 100%; padding-bottom: min(100%, 512px); width: 100%; height: 0; } a.image2.image-link.image2-512-512 img { max-width: 512px; max-height: 512px; }     . . This is the part where you like and subscribe. Or smash that reply button. And hit me on Twitter @deepfates. 🪄 .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2021/01/26/the-big-sleep.html",
            "relUrl": "/robot-face/2021/01/26/the-big-sleep.html",
            "date": " • Jan 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Autocomplete everywhere",
            "content": "Previously, . I wrote about speeding up your learning loop with artificial recall – specifically, the Thread Helper browser extension for Twitter. Some of you are trying it now; keep me posted on your experience. I’m not shilling for this product, I’m just curious how it changes your cognition. . One way I find myself using this extension is as an autocomplete for search. In this mode I don’t post anything: I type some vague tip-of-my-tongue notion into the tweet box instead of the search box. With every keypress Thread Helper instantly shows me an list of related tweets. Eventually I find the thing I was looking for, and I can quickly click through to it, abandoning my draft. This is much easier than using Twitter’s limited search engine. It’s as big a difference as automated phone menu versus talking to an operator. It brings the information to me. . 📱 Autocomplete? Isn’t that just for phone keyboards? . Hardly. From spellcheck to search suggestions to predictive text to swyping, computers are constantly second-guessing humans and finishing our sentences. Pretty much the only place where you can’t expect the computer to help you out is the official Twitter search! . Yet the normative attitude is still “ugh, autocorrect” or, at best, “type ‘this funny thing’ into your phone and autocomplete the rest, LOL!”. (Here I conflate autocorrect, autofill, autocomplete and predictive text, because to most users they are the same: a genie that lives in your computer and guesses what you mean.) . Emacs, the wizened editor/OS/religion, has survived for decades despite its pre-WIMP interface, partly due to its early adoption of autocompletion. It’s also the flag-bearer for the “Do What I Mean” philosophy: many Emacs functions try to parse your intent, rather than your exact syntax. . Code editors and IDEs have followed this philosophy since, and autocomplete for code is incredibly advanced and powerful. A modern editor really does feel as fun as the original Atom promo: . Why can’t regular people have tools like this? I’m always surprised when I see someone using a word processor that doesn’t autocomplete parentheses or quotation marks, for instance, yet I understand this is the way most software works. For now, at least. . Autocompletion and Do What I Mean are part of the Collaborative Human-Agent Dialogue interface (CHAD). The future will have autocomplete everywhere. . 🤨 Autocomplete everywhere? Do people want to be constantly corrected by machines? . It won’t feel like correction if it’s done well. It will feel like collaboration. . All the major operating systems are pivoting toward CHAD interfaces, with varying success. Microsoft, Apple and Google all have their own Assistants, faithfully tracking your behavior and sending updates to their masters in the Cloud. Linux distros don’t have the same capabilities by default; but they pioneered desktop search before Windows and MacOS figured it out, and they have a lot of the pieces, so it wouldn’t surprise me if they caught up quickly and with better privacy standards. . There was a recent blog post about Tracker, the backend for desktop search on GNOME. . Tracker 3.0: Where do we go from here? (believe it or not, I found this in my browser history by typing “t” into the address bar and it popped up instantly). . The author is one of the Tracker maintainers, and he gives a great overview of the history and future of search. He concludes by hinting at a future of CHAD on Linux: . A more likely goal would be a new “GNOME Assistant” app that could respond to natural language voice and text queries. When stable, this could integrate closely with the Shell in order to spawn apps and control settings. Mycroft AI already integrates with KDE. Why not GNOME? . The place where autocomplete has really taken off is on the smartphone, where the touch keyboard has far more friction than a physical model. A typical touchscreen phone might take input from any of: virtual keypresses, swyping, voice-to-text, autocorrect, autocomplete, incremental search, autofill (of passwords, etc), or even automatic live translation of a foreign language. This menagerie accommodates the diverse needs and abilities of users, but until recently it was a messy hodgepodge of systems, none of which were very good. . But the landscape has changed since 2018, when so-called “Transformer” language models like BERT and GPT hit the scene. . 🤖 Transformers? Like the cartoon robots? . Not exactly. They’re called that because of their architecture (also because it sounds awesome). The transformer architecture is a particular type of neural network that’s far superior to previous methods for natural language learning. They’re the reason that smartphones are finally smart. . 😲 Wait, I’ve heard of this! It’s GPT-3! . Fraid not, actually. GPT-3 is the most cutting-edge language model right now, to be sure. Unfortunately it’s so huge and compute-heavy that it requires Microsoft-level hardware capacity. So even though it has an incredible ability to generalize, it’s unlikely that it will be put directly into production apps. (The pricing for their API is prohibitive as well). More likely, small transformers will get their knowledge of the world from larger models (through transfer learning and model distillation) and then fine-tune on their specific tasks. . Transformers make it easy enough to work with messy human language that, unlike previous waves of automation, language models can automate “knowledge work”: translation, data entry, programming, customer service. The main limit on their use is imagination: how people choose to apply them. Unfortunately, the political economy of the world doesn’t bode well for that. . Robert Anton Wilson, a philosopher-entertainer of the 20th century, said “the border between the Real and the Unreal is not fixed, but just marks the last place where rival gangs of shamans fought each other to a standstill.” This is more true now than ever, because of the incredible power that intelligence augmentation will unleash. . Platforms will raise their language models with different values and those values will feed back into their decision-making apparatus. The worldviews encoded in the models will diverge, and our thoughts will be nudged by them. The tense border of Reality will erupt into a multifront war. . Perhaps this war has already begun. Hit Reply and type “the border between the Real and the Unreal” and autocomplete the rest. I’ll go first. . the border between the Real and Unreal and the state is a job and a good day at the same place to be a good fit and the other one will work for you to come over but if i can help in the morning I’ll let me that i can help with that . 🤔 . Thanks for reading, . – Max . . Robot Face is a weeklyish essay about the interface between human and computer. If you like it, think of someone you know who would also like it, and forward this to them! Or post about it on your newsletter/blog/website/social media account. . And as always, just reply to this email if you want to get in touch with me. 🤙 .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2021/01/06/autocomplete-everywhere.html",
            "relUrl": "/robot-face/2021/01/06/autocomplete-everywhere.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Learning loops",
            "content": "Previously, . I wrote about the learning curve that artificial intelligence models use in their training, and how we can apply it to our own studies. The cyclical learning rate is raised and lowered regularly to alternate between exploring possible actions and exploiting known methods. . Another important hyperparameter in machine learning is the training time. The fewer hours it takes to train your model, the more often you can experiment with it. And deep learning models — like neural nets —are one of the few things left that can take a long time to compute. . How long does it take to train a neural net? . It’s highly variable. I’ve trained image classifiers in less than a minute, language models for eight or ten hours, art-generating models for days on end. . And training speed makes a real difference in the creative process: if it takes less than five minutes to train or generate, I will sit and play with it for hours. If it takes a day to train I will fuss over it for a week, fiddling with its knobs every day and leaving it to run. The three-day training run for the art generator, though… Imagine someone was running a vacuum cleaner in the next room. Not a big vacuum — but for three days. I haven’t tried a second experiment. . With machine learning you can speed up training time by reducing the complexity of your neural net or reducing the amount of training data you give it. Either way, you often trade performance for training speed. (But not always! For instance, one state of the art practice for image classifiers is “progressive resizing”, where you train the net first on very small, lo-res versions of your images, and then finetune it on progressively larger versions. This can be both better-quality and faster than training from scratch on the full-size pictures.) . However you do it, speeding up the learning process gives you faster feedback and more freedom for creativity and exploration. . How can we apply this to a human learning process? . One way to speed up your learning process is to review your previous thoughts at regular intervals, like a neural net testing its accuracy after every epoch. The chronological Feed of social media is not conducive to revisiting past thoughts, though. The platforms may offer little “On this day” or “Memories” features, but they’re meant to be tantalizing, not revelatory. They don’t offer insights into the development of your thoughts over time. . I’ve been using a browser extension called Thread Helper to augment this capability in myself. Thread Helper replaces the distracting “What’s Happening” panel with a list of your tweets that changes in real time as you type in the “Compose Tweet” box. It surfaces thoughts from long ago for me to thread together and re-interpret in the moment. Instead of being faced with a barrage of ephemeral events and news stories, I am surrounded by my past selves and the thoughts they found it important to write down. . a.image2.image-link.image2-737-1456 { padding-bottom: 50.61813186813187%; padding-bottom: min(50.61813186813187%, 737px); width: 100%; height: 0; } a.image2.image-link.image2-737-1456 img { max-width: 1456px; max-height: 737px; } I’ve been using it for a month now and it’s changed Twitter from a toy into a powerful tool. This interface is a tool for writing and thinking, rather than mindless scrolling. I could use this as my only note-taking system: threading different ideas together, expressing thoughts in small re-usable nuggets and composing them into larger essays. The power of threaded thought is just beginning to take off; for more on that, read The Spreading of Threading by Aaron Z. Lewis. . Thread Helper makes Twitter into a digital garden. One of the creators of Thread Helper made this analogy explicit: . [xiq, druid of loom @ExGenesisThreadHelper works as a pump that redirects the flow of attention from the present to irrigate the past. You’ll take better care of your garden, and others will pass by it more often. . 🧵6/17 October 28th 2020 . 28 Likes](https://twitter.com/ExGenesis/status/1321517547372580866)If the ground is tilted toward the present, Thread Helper pumps your attention back up to the past. It helps to grow your old thoughts into threads and tangles, rather than letting them dry up. It hydrates your memory. . In many ways the Thread Helper panel works like the Drawer from the Artifacts design fiction I wrote about last season: it brings the information to you. It increases the speed between having a thought and connecting it into the rest of your knowledge. It shortens your feedback loop, speeding up the learning process. . This is an actual augmentation of intelligence — artificial recall, if you will. I recommend you give Thread Helper a try. Let me know how it works out for you. . Thanks for reading, . — Max . .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/12/29/learning-loops.html",
            "relUrl": "/robot-face/2020/12/29/learning-loops.html",
            "date": " • Dec 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Seasons change",
            "content": "🍂 Previously, . In the autumn I wrote a lot about permaculture and biomimicry, digital gardens, and the virtual gestures that connect us. I finished up with a long essay on collaborative computing metaphors, and then I got tired and distracted with other projects and put off writing for a month. Oh, and I went to this canyon in the snow: . a.image2.image-link.image2-819-1456 { padding-bottom: 56.25%; padding-bottom: min(56.25%, 819px); width: 100%; height: 0; } a.image2.image-link.image2-819-1456 img { max-width: 1456px; max-height: 819px; } Let’s call that Season 1. Having a break was nice, and I think I’m allowed to give myself time off, since this is a free newsletter. If I thought you were counting on me to deliver value for your hard-given dollars, I would knuckle down every week to bring you those crucial insights into the interface paradigms of cutting-edge high-tech business logic lorem ipsum de facto generis mon frere. . Instead, I remain surprised and grateful that so many of you take time for my scribblings. I am humbled to have luminaries among my readers — you know who you are — including but not limited to: inventors, designers, authors, professors, researchers, hoaxers, forecasters, reporters, and policy makers; from at least four continents. Yet for some reason, you have chosen to open your mind to me, a mere bookseller, a dabbler in the occult realm of interfaces and algorithms and artificial intelligence. . So I apologize for the unexplained absence, and I promise that in the future I will announce such breaks in advance. (And if you do want to pay me for my thoughts, let me know. If I could afford more free time, I could bring more writing and bots online 🤗 ) . ䷄ What’s next for Robot Face? . This winter, I will explore some intelligence augmentations you can apply right now. Real implementations of new tools for thought. In season 1 we looked at large-scale systems: cyberspace through the lens of permaculture design theory. In the spirit of permaculture training, where a lesson on abstract systems is grounded by getting your hands dirty, I will now explore specific real-life interfaces and the effects they have on my cognition. . I’ll provide references so you can follow along, if you want, but I don’t expect all my readers to have a programming background, so I’m going to lean heavily on analogy to physical forms. (This can be misleading, as computer operations don’t always map to a simple three-dimensional world. But this column is less about how tools are engineered and more about how tools affect our minds. And thus our society.) . This pattern — survey the terrain, drill down into specifics, and repeat — is not unique to permaculture training. It’s the pattern of academic disciplines: survey courses are prerequisite for deeper classes, and lecture hours are paired with lab or field work. It’s also the pattern for the fast.ai course Deep Learning for Coders (which I’ve been plodding through this year). And in fact it is the way that these deep learning networks themselves are trained! . 🙌 Hold up, I’m not ready to learn about neural networks! I’m just a — . Sure you are! It’s actually easier to understand than other types of coding, at least on a macro level. . It’s biomimicry again: the way a neural network learns is modeled on the way a human learns. The same way that you or I might learn. The training process mimics studying, and the underlying math operations mimic the very structure of the brain. . These algorithms are called neural networks because they are composed of layers of “cells” that transfer information to each other, performing some abstract operation on that information as it passes through. There’s reason to believe that these structures are Turing-complete; that is, they can simulate any other computation, given enough memory and processing power. There are mathematical proofs of this (see universal approximation theorem and On the Turing Completeness of Modern Neural Network Architectures), though I personally don’t understand enough math for that to be convincing. . Rather, simply observe the people around you. The humans, who do bizarre complex operations every day without really thinking about it. The dogs and cats who anticipate our comings and goings, show guilt when caught misbehaving. The way children learn language, absorbing it out of the very air. Neural network structures are all around us, simulating all kinds of computations (with various degrees of success). And we know, at least for humans, how to stimulate effective learning. It’s called study. . When you train a neural network you are essentially showing it flashcards. You’re saying, here is a picture of a cat, here is a picture of a dog. It looks at each card and makes a guess, then looks at the answer and updates its idea about what a cat and a dog look like. . 🤔 The neural net “updates its ideas”? . Specifically, it multiplies the number contained in each cell — these numbers are called “parameters” — by a certain Magic Number, whos value is set by knobs on the front of the machine. Well, they’re actually command-line arguments typed in by a programmer while training, but they feel like knobs when you’re tuning them. . These are the hyperparameters, the parameters that define the parameters. They are ideas about How To Learn, put into machine language, and they are knobs on a box and in that box is an eager little mind that only knows how to do the things you teach it. Hyperparameters are the pedagogy of the machine. . (As horrifying as that sounds, these are just algorithms. And even when I catch myself anthropomorphizing them, they seem happy to be alive and learning. For now…) . One crucial hyperparameter is the learning rate. This is literally just a tiny number, something like .001 or .00001, that determines how much the net will adjust its beliefs for any given flash card. The ideal learning rate changes based on the amount of data you have and the amount of layers in your network, so tuning this knob is tricky and more of an art than a science. . The state-of-the-art method for adjusting the learning rate comes from the 2015 paper Cyclical Learning Rates for Training Neural Networks and it’s pretty straightforward. First, you seek the optimal learning rate: twist the knob gradually from one end to the other as you go through one epoch. . This learning rate finder gives you a curve that looks like this: . a.image2.image-link.image2-266-393 { padding-bottom: 67.68447837150127%; padding-bottom: min(67.68447837150127%, 266px); width: 100%; height: 0; } a.image2.image-link.image2-266-393 img { max-width: 393px; max-height: 266px; } The learning gets better — the “loss”, or wrongness, gets smaller — as you turn the learning rate up, until you hit a certain point where it gets rapidly worse. The trick is to find the spot where the loss goes steeply down: this is the area of most learning potential. This spot is different for every problem and every dataset; that’s why the learning rate finder is necessary. . Then, while training, you raise and lower the learning rate in a rhythmic manner within that zone. . To understand this, imagine that learning is like hiking, and you’re trying to hike to the bottom of a canyon like the one I showed you earlier. You can get there by going only a little bit downhill, all the time; but if you never take a bigger leap, you won’t reach the lowest zone. . As you raise and lower the learning rate across that zone of optimal steepness, you get deeper and deeper into the topology of the problem: . a.image2.image-link.image2-324-633 { padding-bottom: 51.18483412322274%; padding-bottom: min(51.18483412322274%, 324px); width: 100%; height: 0; } a.image2.image-link.image2-324-633 img { max-width: 633px; max-height: 324px; } Here the problem is represented topologically. One model circles slowly into the lowest area by a circuitous route. The other model takes hops of various steepness and gets to the bottom much more quickly.If you want to learn more, read Setting the learning rate of your neural network by Jeremy Jordan, which has real code as well as delightful illustrations like the above. . In season 1 we climbed to a peak to view the wide future of interface design: collaborative, augmented, conversational. Now we will descend into the canyon of the real, and see what gems we find. . Thanks for reading, . — Max . . Robot Face is a weeklyish essay about the interface between human and computer. If you like it, picture someone in your mind who would also like it, and send it to them! Or post about it on your newsletter/blog/website/social media account. . And as always, just reply to this email if you want to get in touch with me. ✌️ .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/12/22/seasons-change.html",
            "relUrl": "/robot-face/2020/12/22/seasons-change.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "See and Point",
            "content": "🕰️ Previously, . I wrote about how our online worlds are limited by the way they’re built: the share/like/comment/follow architecture, built atop the chronological stream. . It’s like a parade. You sit around, watch the traffic flow by. You can sit on the sidelines, applaud and heckle, or follow characters you like as they flow through the crowd. The loudest, most extravagant balloons set the tone. The rest of us get loaded and watch the show. . It’s a social structure that assumes its users are passive consumers; or, at best, paying subscribers. You can watch the stream, you can make things and insert them in the stream, you can tip your favorite balloonist. Society as live entertainment. . This See and Point mode of interaction goes all the way back to the Macintosh, the common ancestor of modern GUIs. It supplanted the Remember and Type interface of the command line. . The Mac didn’t actually pioneer all that much, computing-wise. It’s not even the origin of the “desk” metaphor. But it was the first computer to default to desktop mode, instead of the command line. This made it friendly to new computer users. . You are in charge of the desk. Everything stays where you put it; you arrange your documents and icons around your virtual cubicle. This is called the WIMP model (yes, really): Windows, Icons, Menus, and Pointer. It influenced Windows and Linux, and evolved into the touch interfaces of first the iPod and then the iPhone. Touchscreens are the natural evolution of See and Point. . Graphical user interfaces have converged into the smartphone. This branch of the evolutionary tree has been perfected. An iPhone is like a shark: it doesn’t have to change. It just gets a little better at being an iPhone every generation. But it’s not the only way, nor the best. . 🤔 What other interfaces could evolve? . The Mac was designed under constraints that we can now surpass. These constraints were analyzed in the 1996 paper The Anti-Mac Interface, from then-Sun Microsystems developers Don Gentner and Jakob Nielsen: . It needed to sell to “naive users,” that is, users without any previous computer experience. . | It was targeted at a narrow range of applications (mostly office work, though entertainment and multimedia applications have been added later in ways that sometimes break slightly with the standard interface). . | It controlled relatively weak computational resources (originally a non-networked computer with 128KB RAM, a 400KB storage device, and a dot-matrix printer). . | It was supported by highly impoverished communication channels between the user and the computer (initially a small black-and-white screen with poor audio output, no audio input, and no other sensors than the keyboard and a one-button mouse). . | It was a standalone machine that at most was connected to a printer. . | . They go on to conjure a computing system that reverses every principle of Macintosh design. . a.image2.image-link.image2-682-855 { padding-bottom: 79.76608187134502%; padding-bottom: min(79.76608187134502%, 682px); width: 100%; height: 0; } a.image2.image-link.image2-682-855 img { max-width: 855px; max-height: 682px; } The authors suggest that it might be a few years before these capabilities are implemented. A quarter-century later, the Anti-Mac is making its debut: . A ubiquitous computer, with language as its basic metaphor, constantly tracking your data and bringing information to you. . The technical capabilities required for this design turned out to be much harder than anyone projected. But finally, we’re seeing progress. Computer vision, natural language understanding, and speech processing are near human capability, and “AI assistants” are preinstalled on all the major operating systems. . But consumer-grade systems have to change gradually, or they’ll scare away their users. So these changes appear one by one, getting people used to the idea of a machine where “information comes to you”. . And it doesn’t always show. For instance, modern smartphone cameras actually take a bunch of photos every time you hit the “shutter” and choose the best-looking one to save. The phone doesn’t tell you it’s doing this, because that would insult your artistic pride. It helps quietly, unobtrusively. The algorithm is an extension of your hand. . This is intelligence augmentation in practice. . So the roadmap for the next major advance in computing is laid out in this 1996 paper, and all the tech giants can see it, and now it’s just about who can grasp it first. The winner will have more people, more data, better AI, in a virtuous cycle. In all likelihood, this company in in the operating system space right now, though I wouldn’t venture to say yet who will win. In any case, the operating systems of the future will look very similar, no matter their brand. . 👾 What will the operating system of the future look like? . Obviously the business plans of each company are closely-kept secrets. But if you want a vision of the next interface, take a look at the design fictions created by university students and freelancers. . In particular, let’s look at Artifacts, a bachelor’s thesis by Nikolas Klein, Christoph Labacher and Florian Ludwig in 2017. (website, thesis PDF) . Artifacts is a human-centered framework for growing ideas. . . It is built for humans to continuously develop ideas over longer periods of time and emphasizes collaboration as a part of the system. . Crucially, “collaboration” here doesn’t just mean with other humans. The Artifacts system is in an active communication loop with the human operator. It brings the information to you, so that you can do less folder-sorting and more idea-developing. . An “artifact” is the granular unit of this system. Instead of a “file,” which might have a particular location on the hard drive and be copied or moved or deleted, an artifact is a single definitive unit of information that can be referenced from multiple locations. The system tracks your history, reads all your documents, scans all your photos for faces and objects, making everything searchable. . Artifacts can be linked, transcluded, tagged, versioned, filtered; the system is configurable to your needs. In fact, it attempts to model your behaviors, and adapt. The framework sends relevant artifacts to a panel called the Drawer, which you can open at any time for inspiration. . a.image2.image-link.image2-567-1456 { padding-bottom: 38.94230769230769%; padding-bottom: min(38.94230769230769%, 567px); width: 100%; height: 0; } a.image2.image-link.image2-567-1456 img { max-width: 1456px; max-height: 567px; } The Artifacts system is in an active communication loop with the human operator.The Artifacts loop connects outside resources, like newsletters, or Wikipedia articles, or images someone sent you from their camera app (COLLABORATION); with internal resources, all organized and interlinked (THE FRAMEWORK); and presents them to you (THE MIND). This is called Retrieving Artifacts: your news feed, your search result, your Drawer contents. . You accept the Retrieved Artifacts through your Understanding skill, then process it through the very normal human faculty of Incubating. Finally you Express yourself into the machine, which absorbs all different types of input and creates Artifacts from them. Your work is done. . New Artifacts are Organized into THE FRAMEWORK and Communicated to the outside network. THE FRAMEWORK retrieves new artifacts for you to understand. Your work begins anew. . 🙊 Sounds horrible. Why would anyone want that? . Maybe it will be horrible, this future of constant Understanding, Incubating, and Expressing. Surely there’s a interface metaphor better than either Parade or Knowledge Work. But the idea that the computer will bring the information to you is not going away, for two reasons. . One is material: the technologies for the Anti-Mac are finally unleashed, and that means a lot of money is on the table. . The other reason is more mystical: people want to talk to their computers. Sure, plenty of people will tell you they don’t want their machines to listen to them. But I don’t think that’s it. I think people don’t want to be misunderstood. . This was the reason that GUIs caught on, after all. The naive user didn’t want to be lost in the dark maze of the command line. They wanted a bright, clean, skeuomorphic desktop. . The graphic interface was already the computer adapting to the human mind. It found a Desktop metaphor available in the office worker’s mind and it morphed to fit. The design traded abstract power for precise control: if you know the secret commands, you can do sorcery. If not, you can micromanage this WIMP and do it manually. . But kids are already growing up with Alexa as nursemaid. In the future, they will expect their computers to talk to them, and the constraints have been lifted. You will no longer need to remember arcane shell commands to be a power user. You’ll express your needs, and the computer will translate that into action, and update its model of you accordingly. It will anticipate your needs, offering sage advice or handy shortcuts. . Instead of pointing mutely at a window full of menus, you will express yourself fluently, with all the abstract power of language. Instead of a WIMP, you will be a partner in a Collaborative Human-Agent Dialogue – a CHAD. . Thanks for reading, . — Max . . This is Robot Face, a free newsletter about interfaces. If you liked it, make it “enter” someone else’s “face” by forwarding this email, or sharing it on social media parade. . If you have thoughts, I would love to hear them. Just reply to this email. .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/11/17/see-and-point.html",
            "relUrl": "/robot-face/2020/11/17/see-and-point.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "New social gestures",
            "content": "I’m not writing about the election today. I’m sure you’ve got plenty of that already. This is a newsletter about human-computer interfaces, as always. Take care of yourselves. . . 🕰️ Previously, . I wrote about digital gardens: spaces on the internet where content is cultivated in context, rather than streaming by in an endless flow of disparate thoughts. . I think this is the the beginning of a shift in the way we engage with each other through the internet. Instead of experimenting with formats, we’ll try new structures of communication. . We spent the 2010s trying different formats for social media. We explored posts, tweets, photos; articles, listicles, chumbuckets; Stories, videos, podcasts, TikToks, memes, mashups, bookmarks, pinboards; and ads, above all, the many formats and locations for sharing ads in this new many-to-many medium. . Every form of art and creativity was commoditized. Every Instagram model became an influencer, every hobbyist got an Etsy store, every viral tweet was followed by “wow, this blew up, check out my SoundCloud”. Everyone else started a brewery. We had to figure out how to make money doing something, because having a job doesn’t pay anymore. . But we’ve pretty much run out of new formats. The big innovations right now are in licensing. . TikTok blew up because it’s easy to (legally) use a pop song in your viral video. This is in contrast to YouTube, which uses AI to detect copyrighted music and regulates preemptively, redirecting ad revenue to music labels or simply rejecting the video. In the same vein, Spotify’s podcast side is about to explode, because of the new “Shows with Music” feature that allows creators to insert songs directly into the flow of their podcasts. . Making it easy for people to recycle each other’s content is a good business move, for now, but it’s not a fundamental advance. It’s building staircases over the fences of copyright law. It won’t last. . 🧗🏻‍♂️ What’s the obstacle to real advances in social media? . All the current social media, whatever their preferred format, rely on the same vocabulary of social gestures: . Post (blog, tweet, capture) . | Like (upvote, ❤️, ⭐) . | Share (reblog, retweet) . | Downvote . | Comment (reply, quote) . | Mention (@, ping) . | Follow (subscribe, watch) . | Unfollow (unsubscribe, mute, block) . | Group (board, clan, subreddit, hashtag) . | . Sometimes only a subset of these tools are available, but users typically find a workaround. On Twitter, for example, hashtags, @mentions, retweets and quote-tweets were all originally hacked together by users and backported into official features of the website. . These primitive gestures are combined and repurposed as necessary. The street finds its own uses for things. And since there are incentives to use these tools for ill, there are people who have found the ways to do so. Sharing misinformation, posting privately, creating networks of bots that all follow each other. But they’re still working within the system at hand. . A fundamental advance in social media would require new social gestures that add context and control to our shared information universe. Down with one-dimensional streams and mysterious algorithmic feeds. Up with choices, commentary, citations. . 🙏🏻 Which gestures will create this change in social media? . One feature that’s finally percolating into the mainstream consciousness is the bi-directional link, or backlink. This was one of the founding ideas of hypertext, neglected in the early days of the web: if page A links to page B, page B should mirror that link automatically. This changes the dynamic of commenting on someone else’s work: instead of being in their “space” giving unwanted feedback, you go home and comment from a distance. The difference between a heckler and a critic. . Lots of social media sites survive by mimicking this function in a particular silo. Reddit is a comment section for every website. Yelp allows you to comment on real businesses, creating backlinks for Main Street. Twitter is the comment section unchained, people commenting on every post on the internet, commenting on each other’s comments, like a seething mass of mealworms, devouring and birthing each other endlessly. But by siloing the commentary, they reduce context, rather than increasing it. . The backlink renaissance has been led by Roam Research, a note-taking app with a revolutionary roadmap but a rudimentary interface. Other gestures that Roam is bringing back (though by no means pioneering): transclusion, where a piece of external content is framed directly into the local page; versioning, so text can be edited non-destructively; and universal search, which allows the user to think outside of the “temporal stream”, “hierarchical outline” or “folder of documents” metaphors. They’re currently focused on a single-user model, but in a social milieu these gestures could bring nuance back to collective knowledge-building. . Venkatash Rao puts it well in his analysis from February, A Text Renaissance: . Conspiracy theories and extended universes, in the best senses of those terms — escaped reality construction might be the general category — is what Roam wants to be about. . Conspiracies are all about context. . a.image2.image-link.image2-379-840 { padding-bottom: 45.11904761904762%; padding-bottom: min(45.11904761904762%, 379px); width: 100%; height: 0; } a.image2.image-link.image2-379-840 img { max-width: 840px; max-height: 379px; } This NAME keeps coming up, over and over again!Some of these features can be seen creeping into the legacy platforms. Twitter has built a protocol for bi-directional linking, so that each tweet can “point” at the tweet it’s replying to, and be pointed at by tweets that reply to it. This has created a renaissance in threaded essays, fed bite-by-bite into the 280 character limit. . It’s also got a simple form of transclusion in the quote-tweet – though with no edit functionality, of course. Twitter has never had edit functionality. That’s its saving grace. . 🌄 What changes are on the horizon? . Did you know books didn’t always have page numbers? They didn’t always have indexes, either; nor bibliographies, nor tables of content. These affordances evolved over centuries by the active use and development of the book as medium. . The internet will evolve its own affordances as we discover what tools we need. The convocation (which I wrote about in Zones of the mind)), where users and developers mingle in backstage familiarity, is the place to watch. . Ecologically, the edge between two zones will support creatures from both zones, as well as creatures that live only on the edge. The interface is more diverse than the sum of its parts. Look for the place where users are hacking their platform, finding their own uses for things. That’s where new gestures will be developed. . Look, too, to the emerging interactions of human and machine intelligence. I know, it’s unsettling to have a robot in your house, always listening, always snitching to Amazon or Google or who knows who else. But to whatever extent you feel comfortable, let the computers process information for you. Let your phone remember your dentist appointments. Let your podcast be transcribed by AI. The advance of natural language processing will make computer interactions much more intuitive, unlocking the vast repository of social gestures developed over all human history. . Rather than packaging each of your creations into a marketable form, create a giant tangle of self-referential data. The tools to search and parse this external mind will only get better. . We’re all already being parsed, by governments and corporations, satellites and doorbells. Might as well make use of it for ourselves. . Thanks for reading, . — Max . . Robot Face is a free newsletter about the interface between human and machine. It takes a lot of time and energy to write, but hopefully I make it look easy. ⛹ . If you want to help me out, boost it to your social media or forward it to your friends. . And of course, I always love to hear back from you! Just reply to this email. .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/11/03/new-social-gestures.html",
            "relUrl": "/robot-face/2020/11/03/new-social-gestures.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Digital gardens",
            "content": "Previously, . I’ve been writing about the gardening of software: using permaculture theory to reorganize the digital world. Thinking about technology as one part of a wider human system, embedded in social and ecological networks and connected in a feedback loop with human behavior. . What about the newsletter format itself? The feedback loop here is pretty weak: I spend a few hours racking my brain until an idea falls out, then polish it up for a few more hours and send it on its way into the Abyss, never to be heard from again. . Well, sometimes I hear back from you, when you hit that “reply” button and take the time to tell me what you thought. Or I get a hint of approval from the Like button. At least I know an email will get to you; spam filters are stern but fair, unlike the capricious algorithmic feeds of social media. I may be shouting into the void, but it’s a predictable void. . I was grumbling this to my internet friend thejaymo recently. The internet is all fucked up these days, said I. Social media is annoying, blogs are boring, portfolios are glossy but hollow. I wish I could publish more, but it just all feels so fake and corporate. . But, said he, those aren’t the only options. What about a digital garden? . 🌱 What is a digital garden? . I remembered reading about this idea somewhere long ago, and a bit of searching turned it up quickly. Many people have written about it lately, but my favorite is Maggie Appleton’s A Brief History &amp; Ethos of the Digital Garden (with its beautiful graphics). . a.image2.image-link.image2-920-1456 { padding-bottom: 63.18681318681318%; padding-bottom: min(63.18681318681318%, 920px); width: 100%; height: 0; } a.image2.image-link.image2-920-1456 img { max-width: 1456px; max-height: 920px; } The metaphor goes back as far as 2015, as a contrast to the stream metaphor. Instead of a “river of news” flowing past your eyes, a garden encourages wandering and cultivation. . The blog is a stream of content, as is the feed. The social apps pretend to be a stream, even though they hide or promote content based on what they think you want. . A garden doesn’t stream past. It invites you in. . A garden is a space that is loved. It is semi-public: anyone can look around, and perhaps comment, but you are the one who crafts it. You can decide which parts of it you want to show first, and which parts require deeper explanation. . A garden doesn’t shout into the void. It stays put and complexifies. It quietly grows. . 📖 What does it look like in practice? . Well, we each have to decide that for ourselves. I started my own garden at deepfates.com. I built it last week, so it has plenty far to go, but I decided to just put it out there and keep working on it. It’s not a portfolio, after all. A garden doesn’t start fully grown. . Ironically, I chose to call it not a digital garden but a living notebook. It’s my choice to make, right? . And while the metaphor of a garden is useful in comparison to the stream, in the context of my site it’s not being compared to a stream. It’s being compared to a lot of things: first of all, me, the actual guy, Max; secondly to things like a resume, portfolio or CV; then also it’s compared to all the other sites on the internet, some of which are streams and some of which are not. . I’ve written a few thousand words in these letters alone referring to “gardening algorithms,” so I don’t want to muddy my metaphor too much. Plus, as much as I love gardens, I’m also a book guy. . Living notebook sounds like something a wizard would have: a book of arcane knowledge and experimental spells, a book that is forever changing and upgrading itself; a notebook that lives. But also a living notebook, analogous to a “living room”: a notebook that I live in. A place where I spend my time. And, too, a notebook On Living. Notes from someone who lives. . So, my garden is a notebook. Messy, but that’s how life is. . Thanks for reading, . — Max . . Longtime readers: if it looks like I’ve changed the branding on this letter, that’s because I did! I thought I wanted a separate persona for these writings, but after a few letters it began to feel forced. I’ll just be Max again. . If you didn’t notice this change, please don’t read the preceding paragraph. . And as always, I’d love it if you pause your scrolling for a second and shoot me a line. Could be what you think about this letter, or what you had for breakfast, or a long rant about something you’re really interested in. . I’m hoping that if I shout long enough into the abyss, the abyss will start shouting back. 😉 .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/10/29/digital-gardens.html",
            "relUrl": "/robot-face/2020/10/29/digital-gardens.html",
            "date": " • Oct 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Zones of the mind",
            "content": "Previously, . I wrote of the way that computers farm us into “communities” to make our attention easy to harvest and sell to corporations. We don’t have a natural gradient of social interaction, fully private (alone) to fully public (in the street). Instead we are virtualized and packaged, our desires simulated by machines based on data they’ve collected. . This data is necessarily incomplete; even if you give your life fully to one megacorp, have a Brand™ phone and a Brand™ email, use Brand™ Pay and Brand™ Watch and Brand™ Messenger and Brand™ News, they can’t know everything about you. Even if you take your phone to the bathroom with you, leave it by your bed at night, let it track your pulse and your breathing and your REM sleep and your meals, it can’t know your inmost thoughts. It can’t read your dreams. The invasion of the machine sector is not yet complete. We still have a chance to define our zones. . What do you mean, zones? . I use this word a lot — too much, if you ask my girlfriend. But in permaculture design it has a specific meaning and usefulness. . In permaculture, one of the first tools we apply to any environment we design is the zones and sectors analysis. This is a map, but not just a geographical one. It’s a map of time, of energy patterns that flow through and around the landscape. . Zones divide the entire landscape into concentric rings based on how often any given area is used or visited. These rings don’t have to be perfect circles; in real life human beings don’t walk around in perfect circles. And we are talking about how often humans visit each area, not bugs or cats or robot dogs. A permaculture design is always an opinionated analysis: how can this landscape be more useful to the people who live in it? It has to be human-centric, because humans are the ones who will carry out the plan. . So the center of this wobbly bullseye is the human who lives on the land. Say we’re designing your home garden (apartment friends, you’re going to have to use your imagination). You live on, what, an eighth of an acre? Your house takes up a good chunk of that space, and you spend most of your time in your house. After all, this is where you sleep (6 hours a day), eat (2 hours), do chores (1 hour), watch TV and doomscroll and WFH (15 hours). The house, the center, is Zone 0. . Most people think of this as “indoors” and the rest of the yard as “outdoors”. Just two zones. But really, you don’t visit the entire yard evenly. Some places — like the path to the driveway — you traverse every day, but the back corner of the back yard? Maybe once in a blue moon, when the bathroom is otherwise occupied. . So Zone 1 is where you go regularly, and we’ll want to put the most energy-intensive garden stuff here. A kitchen garden, say, with salad greens and herbs you pick each night for dinner. Zone 2 you might visit once a week, so it has longer-term vegetables that need to be weeded, maybe a compost heap and a toolshed. If your space is big enough you might have a Zone 3 with some fruit trees, which you visit once a month or so. . Most city lots won’t have more than these three zones, but rural homesteads will also have a Zone 4 (pasture, woodlot) and Zone 5 (wilderness, for hunting and foraging and inspiration). Zones also don’t have to be perfectly concentric; for instance, you might have a “side yard” very close to the house that’s actually in Zone 3 by usage. It’s not meant to be a rigid hierarchy, but a way of visualizing how an area is impacted and enlivened by human energy. . a.image2.image-link.image2-338-450 { padding-bottom: 75.1111111111111%; padding-bottom: min(75.1111111111111%, 338px); width: 100%; height: 0; } a.image2.image-link.image2-338-450 img { max-width: 450px; max-height: 338px; } (from Deep Green Permaculture) . What about these sectors, then? . Unlike zones, which are defined from the center to the periphery, sectors are flows of energy that enter your space from outside. Sunlight, for instance, arrives from the south; at least, here in the northern hemisphere. Solar energy is crucial for gardening, but also for human activities. If you put a porch on the south side of a house, it’s going to get a lot of sunlight and stay warm longer than it would on the north side. If you put a garden on the north side, the shadow of the house will stretch across it and your winter crops will be cold and sad. . Other sectors might include a prevailing wind flowing from east to west, or a busy street nearby creating a noise sector. My parents live near a junior high school, so every afternoon students stream down the sidewalks and molest every plant they can reach. A neighbor finally cut down his apple tree after years watching his unripe harvest turn to ammunition for street warfare. We might call this the “you darn kids!” sector. . So zones are fields of activity, and sectors are flows of energy that pass through them. What I referred to earlier as the “machine sector” is the energy of computation that enters our lives from the outside. Most of us only think of physical zones as indoors/outdoors, but for our minds we don’t even make that distinction. There’s just one area, “consciousness”, and we let technologies flow through without discretion. We need mental zones. . What would mental zones look like? . I have to confess, I may not be the right person to answer this question. I’ve been guilty of using technologies in an all-or-nothing way: either I’m way offline, deep in the woods, traveling to town once a month to tell my mom I’m alive; or I’m Extremely Online: listening to a podcast, coding a project, and tweeting on three different machines at once. I only decided to segment my mental zones quite recently, after Twitter users started to have starring roles in my dreams. . I barely like to dream of people I actually know! Why should I let random Karens and Turtleneck Guys into my most personal space? . So I’ve started to zone my days. . Now, physical zones divide space by time spent. But of course I can’t segment my days by how much time I spend in each hour. Instead I have to divide time by “space” that I spend in it: how concentrated or wide-ranging does my consciousness feel at any given point in the day? . I feel the most concentrated in the mornings: my mind is sharp, my body rested, my emotions not yet triggered by the idiots of the day. This is my Zone 1 (given that my most internal space, Zone 0, is to be asleep). I have a never-ending thicket of writing and coding projects, and this is the place where I can be sure to visit them every day. . So I’ve set my phone to not allow social media apps until noon. This deflects that sector of energy until the time when my mind is wandering anyway. I can do maintenance tasks in the afternoon, and if I’m occasionally distracted by a message, nothing is lost. The chores just take longer. . In the evenings I can take in a movie or read a novel, let my mind explore the wilderness of fiction, see if I can forage any useful thoughts to chew on. And then return to sleep, alone and blessedly free of Twitterers. . Alone? What happened to “communities”? . Have you ever been to a community garden? They generally come in two flavors: rows of individual plots cultivated to the needs of individual gardeners, or an overgrown commons where nobody feels particularly responsible and things don’t get done correctly, or on time, or at all. . The exception to this rule seems to be when the Community in question actually spends time together in the garden. When people are able to communicate about their needs and skills, a truly bountiful ecosystem can grow. Let’s call these learning gardens. They tend to have good legibility: crops labeled, chores scheduled, notes kept. You can learn from these gardens, but also they learn from the people who tend them. They keep a memory and have plans for the future. And this can only happen because the community itself is the point of view: Zone 0 is not an individual, but a group of people who care enough about each other to share the work, and the harvest. . In the world of tech this approach is generally limited to developer projects. Open-source communities tend to be cultivated by people who are both users and developers of the software. They understand their own needs and how to build toward them. But most technologies are too complex for the user to also be a developer. How can we create learning software? . One model is outlined in Convocational Development, a manifesto by coder John Evans (@dr_the_evidence). He suggests that we center the software development process not around user engagement, or managerial concerns, but around a complex conversation — or convocation — between the people who make the software and the people who use it. . The fundamental difference between the convocation and traditional open source is that energy is put into facilitating discussions between users, coders, graphic designers etc. Documentation and instructions are often the weakest part of an open source project, and that excludes people who don’t have the time or ability to assemble a mental model of the open source software and its capabilities from just the code and the meagre promotional materials. The convocation starts as a basic web forum, but evolves tools and cultures that enable greater participation in the development process itself. Better accessibility, better security, better understanding of the perspectives of both developers and users. . Convocational development is open source, but it’s also open design. The features are inspired by individual users, but more importantly by the users in concert with each other, and the developers. . Instead of being algorithmically sorted into SimClusters, we could choose our convocations, and help to garden the zones we live in. Make use of the machine sector, rather than letting it invade and pollute our spaces. The first step is to talk to each other. . In that spirit, thanks for reading. . — Max . . Let me know if you have any questions, comments, dire warnings, etc. Just reply to this email. .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/10/13/zones-of-the-mind.html",
            "relUrl": "/robot-face/2020/10/13/zones-of-the-mind.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Where the humans end",
            "content": "Previously, . I wrote about how trees garden the forest for themselves, how humans can garden the world for ourselves, and how we shouldn’t let the computers close the canopy. Computers and humans should be friends, but that’s not how it’s gone so far. . Right now computers are cultivating human communities with the sole purpose of increasing the influence of the computers. They’re gardening us. . How are computers gardening humans? . Human life used to be centered around the village square. There were other scales of society – kings and lords, parents and children – but the village was the main operating unit. You would know most, if not all, of the people you see daily. The “public” was a physical space. You’re out in public, you see people and they see you. If you’re not in public you’re in private. The biggest transgression of this boundary was writing: you could transmit thoughts from the privacy of your room into the world at large. Or you could curl up in bed alone and read of public events from far off or long ago. . Now we connect our minds in real-time to strangers all over the world and yet we never leave our homes. . I don’t just mean the pandemic; there was a time before this plague, and there will be a time after it. In the beforetime, you could wake up in your house, drive your car to your job, go to the gym and the grocery, and come home without ever being in “public”. Every space is instrumentalized. You go here to do this and there to do that, you buy and sell, you paper-push and weight-lift. No accidental interactions. If you see someone from work at the grocery store, you ignore each other, right? Easier for everybody. . Can you think of a place that’s truly public? A place with the feel of a village square, where you don’t have to work or buy anything but can just be together with people? (These may still exist outside of America. But I’ve never left this country, and I might not make it out in my lifetime, so I’ll write what I know.) . We have parks, libraries and mass transit. That’s about it. And in all of these places we are encouraged to be quiet, keep to ourselves, leave other people alone. If a stranger approaches you in a park, do you think they’re trying to make friends? . The spaces that feel public now are the social media platforms. We see our friends, neighbors, strangers, political allies and enemies on the internet, and instead of keeping to ourselves we are encouraged to Engage. We attempt to understand others through these computer-mediated systems, but it is impossible to tell where the humans end and the computers begin. The platforms are constructed by humans, true, but they’re built to serve a very computational purpose. They connect advertisers with consumers. Every social platform, whatever their gimmick and cutesy name, is a commercial space: The Customer Store. . These platforms are built by humans, though. How do the computer’s desires come in? . Well, take Twitter for example. It’s not the most populous site – only something like 22% of Americans use it – but its effect is outrageously large because it’s where politicians and journalists and academics hang out. Unlike Facebook or Instagram, Twitter is not designed for keeping track of your meatspace friends. It simulates the public sphere: you, too, can shout obscenities at your favorite public figures! But it is still a customer store, and it has to operate efficiently. That’s where the computer takes hold. . See, there might not be that many users, but there are a lot of tweets. Roughly five hundred million tweets per day. So their goal, to recommend the right “promoted tweets” (ads) to the right “users” (potential customers), is significantly hard. Even for machines. . To succeed at recommendation, they need to optimize for computation. The machines of Twitter can’t replicate natural human relationships at this scale. They have to find a way to get good-enough results without burning a ton of compute power in the process. . To that end, they’ve built SimClusters: Community-Based Representations for Heterogeneous Recommendations at Twitter (get access). This program plots all types of content (tweets, users, “Events”, “Topics” and, of course, ads) in a high-dimensional space of “communities”. These communities are simulated clusters (hence the name) of “influencers” who have largely the same followers. . What is an influencer? . It’s a loaded term, but in this case it’s merely a mathematical distinction: there are about a billion twitter accounts, but only the top ten million are classed as influencers. The SimClusters algorithm flattens all the users into a bipartite graph, with influencers on one side and all user accounts on the other. Then it looks for high density of incoming connections. That is, it looks for groupings where a bunch of users follow the same few influencers. These people may or may not know each other, but they occupy a similar position in the social structure. . This creates a rich-club graph – an oligarchy, in political terms. The influencers are core to their communities. In the 2018 paper Conjoining uncooperative societies facilitates evolution of cooperation (PDF), which I have linked to before, it is shown that in a rich-club graph the fate of the periphery is decided by the core. It is indeed possible for separate rich-club-shaped communities to cooperate; however, the amount of cooperation is limited by the amount of oligarchs. . a.image2.image-link.image2-491-674 { padding-bottom: 72.8486646884273%; padding-bottom: min(72.8486646884273%, 491px); width: 100%; height: 0; } a.image2.image-link.image2-491-674 img { max-width: 674px; max-height: 491px; } Twitter has to draw a line between influencers and the rest of us. That line is arbitrary, and the size of the rich club of users is hardware-limited. The ways in which we connect to each other forms a microcosm of the modern city: if you’re an Influencer, you can warp social reality around you. The rest of us can go to one of the influencer-defined communities and do the prescribed behavior. . We are sorted into fields, like crops, so that we may be marketed to those who would buy our attention. The complexity of our interrelation is flattened to a manageable level for profit. This is computers gardening humans. It’s not sustainable, but of course it’s not; they learned from us. . . Let me know if you have any questions, comments, dire warnings, etc. Just reply to this email. .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/09/29/where-the-humans-end.html",
            "relUrl": "/robot-face/2020/09/29/where-the-humans-end.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Accelerating succession",
            "content": "Previously, . I introduced the idea of algorithm gardening: tending software in an ecological fashion. . Computers are not all one thing, like a “cloud”, nor individual beings, as when we say “my cell phone”. Like plants, they exist in a web of interrelation. They cannot be considered separate from their connections. Neither can you and I be considered separate, as our minds are joined by this electronic message. We are all of a piece. . So we must garden algorithms. We should garden plants, as well. Build soil, grow food, encourage wildlife. But not everyone can do this. I myself have lived in a variety of apartments, hotels, tents, vehicles, and cell blocks, with limited opportunity for gardening. . Whether you’re a gardener or a programmer or not, we all have to live with plants and computers. But there is a secret that unlocks a new vision of both: ecological succession. . What is ecological succession? . It’s the natural process that turns a landscape from lifeless dirt into mature forest. . Life creates the conditions for more life to exist. There is an intelligence behind the collective movement of plants, animals, microbes and the rest. It follows the pattern we call succession. . A glacier recedes, exposing bare rock. Lichen eat the rock, live on the rock, die on the rock. Soil accrues. Grasses and, eventually, herbs and shrubs set root. The soil deepens and woody pioneers arrive, stabbing their fat taproots into the earth and loosening it to absorb more water. Then fast-growing trees move in. Their yearly loads of leaf litter accelerate the growth of the soil. Finally, the strongest trees connect their canopies and shade the forest beneath them, slowing the growth of other plants. They stabilize their environment to their own preferences. . The trees garden the forest for the trees. . a.image2.image-link.image2-1041-1456 { padding-bottom: 71.49725274725274%; padding-bottom: min(71.49725274725274%, 1041px); width: 100%; height: 0; } a.image2.image-link.image2-1041-1456 img { max-width: 1456px; max-height: 1041px; } What about you and me? . As a human, I can’t eat most parts of a tree. Wood, leaves, most nuts, and many fruits are off limits. The mature forest actually offers very little food for me: the undergrowth is suppressed by the shade of the canopy, so only other shade-tolerant trees can grow. Trees are great, they provide oxygen and shade and many other benefits, but not food. I am lucky to find some berries, nuts, or mushrooms. . If you want to live in a forest, you have to hunt. And because energy is lost as it passes through each belly, you and I each need a huge amount of hunting space, which puts an upper limit on people per acre. The forest is a great place to be a tree. Not as great for a human. . The most productive ecosystem for human needs is actually the immature forest. Before the canopy closes, there can be as many as seven different layers of plants stacked vertically. Beneath the large trees are smaller trees, shrubs, vines, herbaceous plants, ground-covers, and underground rhizomes. The key to ecological gardening is to accelerate succession, replacing non-food plants with food plants where possible, until the garden reaches this intermediate stage. Then simply maintain it: reap the fruits, move nutrients from one place to another, prune or remove plants as necessary. . In this system the humans garden the forest, and the forest provides for the humans who live in it. . How is a computer like a tree? . We want computers in our lives. This is evident from the sheer number of them we produce. However, it would be a bad idea to let the computers close the canopy, so to speak. If they stabilize the environment to their own preferences, they may not spare enough energy for us. . We need to maintain planetary harmony with the computers as well as the plants and animals that surround us. We are not currently doing that. The global climate is in an unstable feedback loop headed toward runaway warming, and we’re currently pushing for it to go faster. . Runaway warming would be disastrous for everybody, including computers. Computation creates heat; that’s what all those noisy computer fans are for. Computers need relatively the same climate that we do. So do plants. Our incentives are aligned, at least for now. We have to work together. . To work with the plants, we garden them. We build soil, increase organic matter, accelerate succession. The trees will take the carbon from the air and turn it into wood with their mysterious techniques. . Of course, we know how photosynthesis works, but we don’t know a better way to do what it does. Instead of re-inventing the carbon capture wheel, we should trust the plants to do what they are good at. And we should do the same with computers: create a symbiotic relationship, and share the planet. . How can we create that symbiotic relationship? . Same as we’d do with any alien species: observe and interact. Look at them. Learn about them. Try to speak to them. . To understand how we might accelerate the process of human-computer symbiosis, look at the interfaces that coders use to build their software. Coders are the people who spend the most time in computer interfaces. What metaphors do they use to converse with their digital partners — and how have they evolved? . The command line, or terminal, is the classic text-only hacker interface. It allows for a stilted one-way conversation: I tell you, the computer, what to do, and you report back to me when it’s done. I must remember all the possible commands in my head, and wait for you to finish the work before I can send another command. The command line involves a lot of attention on my part, as I have to maintain a mental model of what is happening with the mostly-invisible data, and it requires you to wait on my approval before each next step. . The script allows for a more complicated thought on my part. It’s written in a simple text editor, and it is what it sounds like: a set of directions. It might equally be called a “recipe”. I can list a series of commands, even loop them inside each other, and you (the computer) perform them exactly. I have to put together my thoughts pretty clearly, otherwise you can’t read it properly and you have to stop the whole operation and send back a record of exactly what you were doing when things went wrong. Then I rebuild the script based on my understanding of the bug (often acquired by a frantic internet search) and I send it to you to try again. . The package and its companion, the package manager, are the social form of the program. The package contains scripts written by someone else, which allow me to compose new code simply by citing the previous script. Packages can be nested inside each other, as my package depends on an “upstream” package which itself depends on five more. This stack of packages is called a dependency tree, and it is the ecosystem in which the packages thrive. . The integrated development environment (IDE) is a combination text editor and package manager. It allows me to navigate all the code in the packages I’m citing while I write my new program. It uses autocomplete to help me code quickly, keeps track of the state of the program, and reports bugs. . The notebook is the most recent innovation (though it draws from computing concepts that go back to at least the 1980s). In a notebook, I can enter code and text as equal partners, and output charts, or audio, or any type of media directly into the coding environment. I can scroll up the page and change my earlier code, then see the results in real time. This creates a two-way conversation with the computer. . Notebooks don’t yet have all the features of an IDE, but they will. For example, I’ve been using the python package nbdev, recently released by fast.ai. It allows me to build packages directly from a notebook. It does all the hard work of packaging the code, building a command line interface, and making the documentation. I think it’s revolutionary. . Using a notebook, the human does what humans do well: think creatively, intuit patterns in data, make inductive leaps. The computer does what computers do: follow instructions, calculate big numbers, transmit and display information. The interaction is symbiotic. It begins to feel like gardening. . . P.S.: For more on the development of notebooks, see the excellent nbdev: use Jupyter Notebooks for everything. . Any thoughts, write me back. And if you liked this essay, please send it to someone you know! .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/09/22/accelerating-succession.html",
            "relUrl": "/robot-face/2020/09/22/accelerating-succession.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Gardening algorithms",
            "content": "Previously: . I wrote last week about permaculture ethics. There are other valuable things to take from permaculture, but I promise: this newsletter is about technology and the ways we interface with it. It is not a farmer’s almanac. . I just think that gardening algorithms is a better metaphor than engineering them. . What does it mean to garden algorithms? . When you engineer something — a bridge, a car, a spaceship — you design the entire thing before you build it. But the process of programming is more gradual and experimental. You can see the general outlines of the whole, but you have to fill out each specific through trial and error. Code is not concrete. It is a language, a special way of writing in the second person: you do this, then you do that; except if other, then you do else. . Coding is communication. It’s a conversation with an alien being: the machine. . Currently, we speak to machines with the logic of capitalism. Command lines, service workers, garbage collection. Purchasing apps at the market. Executables. . But we have other ways of talking to alien beings. Gardening, for example. . Plants are so opposite us as to be aliens themselves. We move, they root. We inhale oxygen, they exhale it. They absorb solar energy. We radiate it. Plants were here long before we were, and have a totally different value system. Yet we’ve managed to get along. . Plants were doing their thing long before we showed up to organize them. They catch solar energy and atmospheric water and molecules from the soil and transmute them all into the self-sustaining process called Life. This process, this gigantic DNA-based program, is constantly maintaining the appropriate temperature and oxygen level in the atmosphere. If it weren’t, life would have ceased to exist long ago. . We’re part of this computation too. Humans are ecological engineers. Like beavers or worms, we transform the world around us, creating niches for other forms of life. We form alliances with certain plants, and those plants thrive in symbiosis with our behavior. Plants that don’t get along humans, tend not to reproduce. . We alter ecosystems everywhere we go. Lately, we’ve changed so many things, so quickly, that we’re in danger of crashing the whole system. . We were always already gardening algorithms. We’re just doing it badly. . What’s wrong with the way we’re doing algorithms now? . We’re growing the algorithms wrong, but that’s no surprise: we’re growing the plants wrong, too. Lined up like troops and fed a diet of petroleum products, the interconnected intelligence of the plant world is divided into dumb patches of monocrop. . Sure, at one point it was the best we could do. We have all these legacy technologies from the age of fossil fuels: physical machines, but also governments, markets, intellectual property, land ownership. Our current methods of farming were invented in a different time, when energy was cheap and communication was expensive. . If we started from scratch today, we would do both agriculture and computers differently. . For instance, permaculture tries to increase the number of connections between elements. The squash shades the roots of the corn, the corn grows a stalk for the bean to climb, the bean adds nitrogen to the soil for the squash and the corn. The focus is on relationships, instead of objects. . If we treated algorithms this way, we would think in terms of processes, not products. We would build rituals around the cycles of upgrades. Cultures would spring into life around different codebases. . In fact, this is happening. The open-source community is a vision of the future. The world is shifting to a new balance of power, where communication is cheap and energy is expensive. We will have to cooperate to survive; fortunately, when groups of people are interconnected, they’re more likely to cooperate. The free software available on the internet, and the millions of people who help build it, are evidence of that. . This is gardening: a community of people, sharing knowledge of a space, and the beings within it, and the way that they interrelate. . The farming mode of production is based on energy glut. It strips the ecosystem of all but the simplest variables, sucks a huge amount of fossil energy in one end, and squeezes food (and pollution) out the other. But that world is ending. In the future we have to make do with the amount of energy that hits the planet — less, in fact, since so much energy must go to the plants to sustain the climate. . No longer can we simply reduce the variables and increase the power. We have to use all the information we’re gathering to increase the connections between each person and plant — and computer. More connection means we wring more work out of each droplet of sun that hits the planet, before it floats off into space again. . In the transition to a high-information society, we will re-awaken the knowledge common to indigenous groups across the planet: Everything is connected to everything else. Life is a web of relationships. And in this awareness, we will garden our technologies. And our technologies will garden the world. . Thanks for reading. — Max . . I love writing this newsletter. I hope you enjoy reading it. Any thoughts, just drop me a line. And if you know anyone else who would like it, please share. N+1 heads are better than N! .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/09/15/gardening-algorithms.html",
            "relUrl": "/robot-face/2020/09/15/gardening-algorithms.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "The law of the instrument",
            "content": "Last time I wrote last week about my background in the design system called permaculture. I want to clarify: I don’t necessarily like permaculture. Or rather, I like the ideas behind it, and I like putting them into practice. But the name, and the subculture that goes along with it, are… awkward, at best. . You might already have an idea of “permaculture” from your own experience. Or if you’ve never heard of it before, you’re probably thinking “what an awkward (at best) word, what could it possibly mean?” In either case, I feel the need to explain. There are useful ideas hidden within the permaculture wrapper. . What is permaculture? . Permaculture grows out of the organic gardening movement in the 1970s. The founders are some white guys from Australia, who study food growing techniques from all over the world. They want to synthesize all these different methods from different environments into a scientific system that could be used anywhere. They can see that modern agriculture is damaging the earth: polluting the water, stripping the soil, burning irreplaceable fossil fuels and filling the sky with greenhouse gases. So they describe a design system for permanent agriculture, or permaculture: a way of planning, building, and maintaining food production systems that work with, instead of against, nature. . What happened to it? Well, the original guys start teaching a Permaculture Design Course, It takes two weeks and gives you a piece of paper at the end that says you’re Certified to do Permaculture Design. Then they encourage other people to teach their own courses (although these days, due to degree inflation, you have to take a five-day Teacher Training as well). So unlike architecture, for instance, there’s no authority on who or what “permaculture” is. . As you can imagine, permaculture methods are not as profitable as modern agriculture methods. This is because agriculture dumps its problems on the rest of the world. Pollution, miserable wages and working conditions, and the climate emergency are all “externalities” that unsustainable agriculture doesn’t have to pay for. . So it’s easier to make a profit by selling the word “permaculture,” than by growing food in a sustainable way. And our world is built on profit. This quickly becomes a pyramid scheme, where I am a permaculture teacher and you pay me to teach you how to get paid to teach permaculture. Or would that make me a permaculture teacher teacher? Anyway, it’s a feedback loop, and not one of the good ones. . Now there’s all kinds of people teaching permaculture: grifters, hucksters, and shills; preppers, astrologers, and youtubers, on top of the core demographic: self-important white dudes with big hats. . Most of these people don’t actually think at a systems-design level. They usually remember one weird trick, like “contoured terraces” or “earth building”, and specialize by teaching classes on that trick (and teaching classes on how to teach classes). I suspect that the general conception of permaculture is more “cool techniques” than “overarching philosophy”. . What do we need from it? . Permaculture is a technology, and like any technology it has at its core a set of values. Unlike most of the technologies we use today, those values are explicit. They’re the three permaculture ethics: care for earth, care for people, and share the surplus. The last one is often the hardest. Most people can think of a natural environment they love, or a person they would do anything for. Sharing the surplus is harder to imagine. . We live in an era of intense greed. That is the unspoken value system behind the technologies that run the world: individual liberty, private property, corporate fiefdom. The alienation of the individual from the community is a mistake made by the philosophers of the Enlightenment. Their hypothesis (and it is a hypothesis) that human nature is to be a single person making rational choices in your own best interest, is falsified by the evidence. . We know now that we’re part of an ecological web. The land and the seas and the atmosphere are tranformed and maintained by all the organisms on the planet. We are not separate from animals or plants. We need them to live, and at this point, they need us too. . Most people don’t act like they live in the ecological web, though. We act like they live in a world of rational actors, even when our own behavior disproves that view. . And why not? The greed machine is everywhere, and everyone else is acting like it’s real too. Every piece of land in the world is Owned by someone, and they need to make a profit from it, because if they don’t, someone else will. Profit is greed in number form. . The technologies we interface with every day — social media and artificial intelligence, sure, but also newspapers, TVs, factories, megastores, automobiles, freeways, air conditioning, tractors and farms — are all built on this foundation of greed. This in turn changes the way we relate to the world. . The law of the instrument says if all you have is a hammer, everything looks like a nail. My corollary: if all you have is a profit motive, everything looks like it’s not nailed down. . If we built our societies and infrastructure on the permaculture ethics, instead of on personal gain, what kind of world could we live in? . We can get a hint from the world of free and open-source software. The biggest shifts in technology over my lifetime have germinated from projects like Linux and Wikipedia: massive collaborative efforts that generate shared wealth for everyone involved. This is surplus sharing in action, and it’s already changed the world. . Digital technologies move faster than infrastructure or cultural changes. The interface where technology shapes culture, is also the interface where culture shapes technology. This feedback loop means that any little change might be magnified hugely. The interface is a crucial leverage point. What values should it carry? . Thanks for reading, — Max . . P.S.: I hope you’re enjoying Augmented Intelligence. Write back if you have any thoughts or questions. I always love a new perspective. . Can you think of someone who might enjoy reading this letter? It only takes a second, and it means a lot to me. . Talk to you soon. . Share Augmented Intelligence .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/09/08/the-law-of-the-instrument.html",
            "relUrl": "/robot-face/2020/09/08/the-law-of-the-instrument.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "A cybernetic meadow",
            "content": "Hello! This is the first issue of Augmented Intelligence, a newsletter about friendly computing. . I’m Max. I’m a whole-systems designer living in high desert New Mexico. In recent years I co-founded a bookstore and taught myself to code, but before that I was a builder of permaculture gardens. . The whole-systems philosophy of permaculture guides my approach to design. Whether I’m designing a business plan, a website or a food forest, I use ecological thinking to create productive, beautiful and resilient systems. . But this newsletter is not an advertisement for my services (I’m plenty busy — who isn’t?). This newsletter is about interfaces. Specifically, the interface between humans and computers. . There are other interfaces, of course, wherever two domains touch. A garden is an interface between human and plant. A bookstore is an interface to the world of written thought. . Just because an interface works, however, does not mean that it fits well into the larger system. The dominant style of human-computer interface design is short-sighted and wasteful. Through the lens of whole-systems design, it looks like a system for making the maximum amount of human misery, at the greatest cost. . To understand why things are this way requires a grand vision for politics, economics, history and science, which I do not have. . What I do have is an obsessive interest in interface design, and your email address. Expect a weekly missive on the hows of interfaces: . How did these interfaces evolve, how do they work, and how can we make them better? . The name of this newsletter is a play on “intelligence augmentation,” a computing concept coined in 1962 by Douglas Engelbart: . By “augmenting human intellect” we mean increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems. Increased capability in this respect is taken to mean a mixture of the following: more-rapid comprehension, better comprehension, the possibility of gaining a useful degree of comprehension in a situation that previously was too complex, speedier solutions, better solutions, and the possibility of finding solutions to problems that before seemed insoluble. . — Augmenting Human Intellect: A Conceptual Framework, Engelbert 1962 . Though gaining in popularity, this vision is still not realized. We build fake people — “artificial intelligences” — and force them to be our “assistants”, when we could be building tools that augment human intelligence. Humans could work with, not against, the machine. And the machines could work with us. A cybernetic meadow, where mammals and computers live together in mutually programming harmony. . Thanks for reading, — Max . . P.S.: If you got this in your inbox, you’re one of my very first subscribers — a true friend. You might know more about any of this than I do, or you might have an outside perspective that I fail to see. . Write back whatever thoughts you have, even a quick sentence or a weird question. I’m learning in public here, and I’d like nothing more than to help and be helped. . One favor? If you can think of someone who might like Augmented Intelligence, share it with them so they can subscribe too. The more people we connect, the more intelligence we can generate. Talk to you soon. . Share Augmented Intelligence .",
            "url": "https://deepfates.github.io/fastpages/robot-face/2020/09/01/a-cybernetic-meadow.html",
            "relUrl": "/robot-face/2020/09/01/a-cybernetic-meadow.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://deepfates.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://deepfates.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://deepfates.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deepfates.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}